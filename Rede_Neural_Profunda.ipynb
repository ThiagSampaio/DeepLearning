{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rede_Neural_Profunda.ipynb",
      "provenance": [],
      "mount_file_id": "1fEHXTfNACVDMzDQ1vVLPPdpl5zHmLnrC",
      "authorship_tag": "ABX9TyM3xUfLdCe1fRX0V65Q1ymO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThiagSampaio/DeepLearning/blob/main/Rede_Neural_Profunda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1X_s6DZytKZQ"
      },
      "source": [
        "# Construindo uma rede neural profunda: Passo a passo\n",
        "\n",
        "Vamos construir uma rede neural profunda com inúmeras camadas.\n",
        "\n",
        "**Notação**:\n",
        "- Sobrescrito $[l]$ denota uma quantidade associada ao $l^{th}$ camada. \n",
        "    - Exemplo: $ a ^ {[L]} $ é a ativação da camada $ L ^ {th} $. $ W ^ {[L]} $ e $ b ^ {[L]} $ são os $ L ^ {th} $ parâmetros da camada.\n",
        "- Sobrescrito $ (i) $ denota uma quantidade associada ao exemplo $ i ^ {th} $.\n",
        "    - Exemplo: $ x ^ {(i)} $ é o $ i ^ {th} $ exemplo de treinamento.\n",
        "- Lowerscript $ i $ denota a entrada $ i ^ {th} $ de um vetor.\n",
        "    - Exemplo: $ a ^ {[l]} _ i $ denota a $ i ^ {th} $ entrada das ativações da camada $ l ^ {th} $).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_XoVsNO0E8G"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkv-SJOV0jpu"
      },
      "source": [
        "<a name='1'> </a>\n",
        "## 1 - Pacotes\n",
        "\n",
        "Vamos importar todos os pacotes que usaremos neste projeto.\n",
        "\n",
        "- [numpy] (www.numpy.org) é o pacote principal para computação científica com Python.\n",
        "- [matplotlib] (http://matplotlib.org) é uma biblioteca para plotar gráficos em Python.\n",
        "- dnn_utils fornece algumas funções necessárias para este notebook.\n",
        "- testCases fornece alguns casos de teste para avaliar a exatidão de suas funções\n",
        "- np.random.seed (1) é usado para manter consistentes todas as chamadas de função aleatórias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9dWLPCvtJ_d"
      },
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "from testCases import *\n",
        "from dnn_utils import sigmoid, sigmoid_backward, relu, relu_backward\n",
        "from public_tests import *\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "np.random.seed(1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQITl-uf3D9h"
      },
      "source": [
        "<a name='2'> </a>\n",
        "## 2 - Esboço\n",
        "\n",
        "Para construir sua rede neural, implementaremos várias \"funções auxiliares\". Essas funções auxiliares serão usadas na próxima tarefa para construir uma rede neural de duas camadas e uma rede neural de camada L.\n",
        "\n",
        "Cada pequena função auxiliar terá instruções detalhadas das etapas necessárias. Aqui está um esboço das etapas desta tarefa:\n",
        "\n",
        "- Inicializaremos os parâmetros para uma rede de duas camadas e para uma rede neural de $ L $ -camada\n",
        "- Implementaremos o módulo de propagação direta (mostrado em roxo na figura abaixo)\n",
        "     - Completaremos a parte LINEAR da etapa de propagação direta de uma camada (resultando em $ Z ^ {[l]} $).\n",
        "     - A função ATIVAÇÃO é fornecida para nós (relu / sigmóide)\n",
        "     - Combinaremos as duas etapas anteriores em uma nova função de avanço [LINEAR-> ATIVAÇÃO].\n",
        "     - Empilharemos a função de avanço [LINEAR-> RELU] vez L-1 (para as camadas 1 a L-1) e adicionaremos um [LINEAR-> SIGMOID] no final (para a camada final $ L $). Isso nos dará uma nova função L_model_forward.\n",
        "- Calcularemos a perda\n",
        "- Implementaremos o módulo de propagação para trás (denotado em vermelho na figura abaixo)\n",
        "    - Completaremos a parte LINEAR da etapa de propagação para trás de uma camada\n",
        "    - O gradiente da função ACTIVATE é fornecido para nós (relu_backward / sigmoid_backward)\n",
        "    - Combinenaremos as duas etapas anteriores em uma nova função de retrocesso [LINEAR-> ATIVAÇÃO]\n",
        "    - Empilharemos [LINEAR-> RELU] para trás L-1 vezes e adicionaremos [LINEAR-> SIGMOID] para trás em uma nova função L_model_backward\n",
        "- Por fim, atualizaremos os parâmetros\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1ZJbpt-D3NHnXu9i5_0cnCGjO9WiZk8zK)\n",
        "\n",
        "**Observação**:\n",
        "\n",
        "Para cada função de avanço, há uma função de retrocesso correspondente. É por isso que em cada etapa de seu módulo de encaminhamento armazenaremos alguns valores em um cache. Esses valores em cache são úteis para calcular gradientes.\n",
        "\n",
        "No módulo de retropropagação, podemos usar o cache para calcular os gradientes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVxMt1Sb7xIm"
      },
      "source": [
        "<a name='3'></a>\n",
        "## 3 - Inicialização\n",
        "\n",
        "Escreveremos duas funções auxiliares para inicializar os parâmetros do seu modelo. A primeira função será usada para inicializar parâmetros para um modelo de duas camadas. O segundo generaliza esse processo de inicialização para camadas $ L $.\n",
        "\n",
        "<a name='3-1'></a>\n",
        "### 3.1 - Rede Neural de 2 camadas \n",
        "\n",
        "<a name='ex-1'></a>\n",
        "### Demonstração 1 - initialize_parameters\n",
        "\n",
        "Criaremos e inicializaremos os parâmetros da rede neural de 2 camadas.\n",
        "\n",
        "** Instruções **:\n",
        "\n",
        "- A estrutura do modelo é: * LINEAR -> RELU -> LINEAR -> SIGMOID *.\n",
        "- Use esta inicialização aleatória para as matrizes de peso: `np.random.randn (shape) * 0,01` com a forma correta\n",
        "- Use inicialização zero para os vieses: `np.zeros (shape)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLfoBncG3DmC"
      },
      "source": [
        "def initialize_parameters (n_x, n_h, n_y):\n",
        "  \"\"\"\n",
        "    Argumento:\n",
        "    n_x - tamanho da camada de entrada\n",
        "    n_h - tamanho da camada oculta\n",
        "    n_y - tamanho da camada de saída\n",
        "    \n",
        "    Retorna:\n",
        "    parameters -- dicionário python contendo seus parâmetros:\n",
        "                    W1 - matriz de peso da forma (n_h, n_x)\n",
        "                    b1 - vetor de polarização da forma (n_h, 1)\n",
        "                    W2 - matriz de peso da forma (n_y, n_h)\n",
        "                    b2 - vetor de polarização da forma (n_y, 1)\n",
        "  \"\"\"\n",
        "    \n",
        "  np.random.seed(1)\n",
        "\n",
        "  W1 = np.random.randn(n_h, n_x) * 0.01\n",
        "  b1 = np.zeros((n_h, 1))\n",
        "  W2 = np.random.randn(n_y, n_h) * 0.01\n",
        "  b2 = np.zeros((n_y, 1))\n",
        "\n",
        "  assert (W1.shape == (n_h, n_x))\n",
        "  assert (b1.shape == (n_h, 1))\n",
        "  assert (W2.shape == (n_y, n_h))\n",
        "  assert (b2.shape == (n_y, 1))\n",
        "    \n",
        "  parameters = {\"W1\": W1,\n",
        "                \"b1\": b1,\n",
        "                \"W2\": W2,\n",
        "                \"b2\": b2}\n",
        "    \n",
        "  return parameters"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y14_LjxbrNDY",
        "outputId": "48a52b0d-5546-4077-add8-e4bbe876516a"
      },
      "source": [
        "parameters = initialize_parameters(3,2,1)\n",
        "\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
        "\n",
        "initialize_parameters_test(initialize_parameters)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1 = [[ 0.01624345 -0.00611756 -0.00528172]\n",
            " [-0.01072969  0.00865408 -0.02301539]]\n",
            "b1 = [[0.]\n",
            " [0.]]\n",
            "W2 = [[ 0.01744812 -0.00761207]]\n",
            "b2 = [[0.]]\n",
            "\u001b[92m All tests passed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR7GAp4P-N8D"
      },
      "source": [
        "<a name='3-2'> </a>\n",
        "### 3.2 - Rede Neural de camada L\n",
        "\n",
        "A inicialização para uma rede neural de camada L mais profunda é mais complicada porque há muito mais matrizes de peso e vetores de polarização. Ao completar a função `initialize_parameters_deep`, você deve se certificar de que suas dimensões correspondem entre cada camada. Lembre-se de que $ n ^ {[l]} $ é o número de unidades na camada $ l $. Por exemplo, se o tamanho de sua entrada $ X $ for $ (12288, 209) $ (com $ m = 209 $ exemplos), então:\n",
        "\n",
        "<table style=\"width:100%\">\n",
        "    <tr>\n",
        "        <td>  </td> \n",
        "        <td> <b>Shape of W</b> </td> \n",
        "        <td> <b>Shape of b</b>  </td> \n",
        "        <td> <b>Activation</b> </td>\n",
        "        <td> <b>Shape of Activation</b> </td> \n",
        "    <tr>\n",
        "    <tr>\n",
        "        <td> <b>Layer 1</b> </td> \n",
        "        <td> $(n^{[1]},12288)$ </td> \n",
        "        <td> $(n^{[1]},1)$ </td> \n",
        "        <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td> \n",
        "        <td> $(n^{[1]},209)$ </td> \n",
        "    <tr>\n",
        "    <tr>\n",
        "        <td> <b>Layer 2</b> </td> \n",
        "        <td> $(n^{[2]}, n^{[1]})$  </td> \n",
        "        <td> $(n^{[2]},1)$ </td> \n",
        "        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td> \n",
        "        <td> $(n^{[2]}, 209)$ </td> \n",
        "    <tr>\n",
        "       <tr>\n",
        "        <td> $\\vdots$ </td> \n",
        "        <td> $\\vdots$  </td> \n",
        "        <td> $\\vdots$  </td> \n",
        "        <td> $\\vdots$</td> \n",
        "        <td> $\\vdots$  </td> \n",
        "    <tr>  \n",
        "   <tr>\n",
        "       <td> <b>Layer L-1</b> </td> \n",
        "        <td> $(n^{[L-1]}, n^{[L-2]})$ </td> \n",
        "        <td> $(n^{[L-1]}, 1)$  </td> \n",
        "        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td> \n",
        "        <td> $(n^{[L-1]}, 209)$ </td> \n",
        "   <tr>\n",
        "   <tr>\n",
        "       <td> <b>Layer L</b> </td> \n",
        "        <td> $(n^{[L]}, n^{[L-1]})$ </td> \n",
        "        <td> $(n^{[L]}, 1)$ </td>\n",
        "        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n",
        "        <td> $(n^{[L]}, 209)$  </td> \n",
        "    <tr>\n",
        "</table>\n",
        "\n",
        "Lembre-se de que quando calculamos $ W X + b $ em python, ele realiza a transmissão. Por exemplo, se:\n",
        "\n",
        "$$ W = \\begin{bmatrix}\n",
        "    w_{00}  & w_{01} & w_{02} \\\\\n",
        "    w_{10}  & w_{11} & w_{12} \\\\\n",
        "    w_{20}  & w_{21} & w_{22} \n",
        "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
        "    x_{00}  & x_{01} & x_{02} \\\\\n",
        "    x_{10}  & x_{11} & x_{12} \\\\\n",
        "    x_{20}  & x_{21} & x_{22} \n",
        "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
        "    b_0  \\\\\n",
        "    b_1  \\\\\n",
        "    b_2\n",
        "\\end{bmatrix}\\tag{2}$$\n",
        "\n",
        "Depois $WX + b$ será:\n",
        "\n",
        "$$ WX + b = \\begin{bmatrix}\n",
        "    (w_{00}x_{00} + w_{01}x_{10} + w_{02}x_{20}) + b_0 & (w_{00}x_{01} + w_{01}x_{11} + w_{02}x_{21}) + b_0 & \\cdots \\\\\n",
        "    (w_{10}x_{00} + w_{11}x_{10} + w_{12}x_{20}) + b_1 & (w_{10}x_{01} + w_{11}x_{11} + w_{12}x_{21}) + b_1 & \\cdots \\\\\n",
        "    (w_{20}x_{00} + w_{21}x_{10} + w_{22}x_{20}) + b_2 &  (w_{20}x_{01} + w_{21}x_{11} + w_{22}x_{21}) + b_2 & \\cdots\n",
        "\\end{bmatrix}\\tag{3}  $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90F7ZHPRCz08"
      },
      "source": [
        "<a name='ex-2'> </a>\n",
        "### Demonstração 2 - initialize_parameters_deep\n",
        "\n",
        "Implementação da inicialização para uma rede neural de camada L.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIff7DqX1HBI"
      },
      "source": [
        "def initialize_parameters_deep(layer_dims):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    layer_dims -- array python (lista) contendo as dimensões de cada camada em nossa rede\n",
        "    \n",
        "    Returns:\n",
        "    parameters --   dicionário python contendo seus parâmetros \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    Wl - matriz de peso da forma (layer_dims [l], layer_dims [l-1])\n",
        "                    bl - vetor de polarização da forma (layer_dims [l], 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "\n",
        "    # número de camadas em uma rede\n",
        "    L = len(layer_dims) \n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1]) * 0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
        "        \n",
        "        \n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8Tvfb3NDxYC",
        "outputId": "349e3356-e3a6-462d-e54a-59bda1b55a6a"
      },
      "source": [
        "parameters = initialize_parameters_deep([5,4,3])\n",
        "\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
        "\n",
        "initialize_parameters_deep_test(initialize_parameters_deep)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
            " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
            " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
            " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
            "b1 = [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
            " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
            " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
            "b2 = [[0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "\u001b[92m All tests passed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLKYsBmJEHGO"
      },
      "source": [
        "<a name='4'> </a>\n",
        "## 4 - Módulo de propagação direta\n",
        "\n",
        "<a name='4-1'> </a>\n",
        "### 4.1 - Linear para frente\n",
        "\n",
        "Agora que você inicializou seus parâmetros, pode fazer o módulo de propagação direta. Comece implementando algumas funções básicas que você pode usar novamente mais tarde, ao implementar o modelo. Agora, você completará três funções nesta ordem:\n",
        "\n",
        "- LINEAR\n",
        "- LINEAR -> ATIVAÇÃO onde ATIVAÇÃO será ReLU ou Sigmóide.\n",
        "- [LINEAR -> RELU] $ \\ times $ (L-1) -> LINEAR -> SIGMÓIDE (modelo completo)\n",
        "\n",
        "O módulo linear progressivo (vetorizado sobre todos os exemplos) calcula as seguintes equações:\n",
        "\n",
        "$$ Z ^ {[l]} = W ^ {[l]} A ^ {[l-1]} + b ^ {[l]}\\tag {4}$$\n",
        "\n",
        "onde $ A ^ {[0]} = X $.\n",
        "\n",
        "<a name='ex-3'> </a>\n",
        "### Demonstração 3 - linear_forward\n",
        "\n",
        "Construiremos a parte linear da propagação direta.\n",
        "\n",
        "**Lembrete**:\n",
        "A representação matemática desta unidade é $ Z ^ {[l]} = W ^ {[l]} A ^ {[l-1]} + b ^ {[l]} $."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHU69ZWIDztn"
      },
      "source": [
        "def linear_forward(A, W, b):\n",
        "    \"\"\"\n",
        "    Implemente a parte linear da propagação direta de uma camada.\n",
        "\n",
        "    Argumentos:\n",
        "    A - ativações da camada anterior (ou dados de entrada): (tamanho da camada anterior, número de exemplos)\n",
        "    W - matriz de pesos: matriz numpy de forma (tamanho da camada atual, tamanho da camada anterior)\n",
        "    b - vetor de polarização, matriz numpy de forma (tamanho da camada atual, 1)\n",
        "\n",
        "    Retorna:\n",
        "    Z - a entrada da função de ativação, também chamada de parâmetro de pré-ativação\n",
        "    cache - uma tupla python contendo \"A\", \"W\" e \"b\"; armazenado para calcular o passe para trás de forma eficiente\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = np.dot(W,A) + b\n",
        "    \n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyf-lJ-6Etnq",
        "outputId": "8815b8e5-dacb-4233-af6a-62d458aaf97d"
      },
      "source": [
        "t_A, t_W, t_b = linear_forward_test_case()\n",
        "t_Z, t_linear_cache = linear_forward(t_A, t_W, t_b)\n",
        "print(\"Z = \" + str(t_Z))\n",
        "\n",
        "linear_forward_test(linear_forward)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Z = [[ 3.26295337 -1.23429987]]\n",
            "\u001b[92m All tests passed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4uItkKDJ2FU"
      },
      "source": [
        "<a name='4-2'> </a>\n",
        "### 4.2 - Linear-Ativação para Frente\n",
        "\n",
        "Neste notebook, usaremos duas funções de ativação:\n",
        "\n",
        "- ** Sigmóide **: $ \\ sigma (Z) = \\ sigma (W A + b) = \\frac{1}{1 + e ^ {- (W A + b)}} $. \n",
        "Foi fornecida a função `sigmoid` que retorna dois itens: o valor de ativação\" `a`\" e um \"` cache` \"que contém\" `Z`\" (é o que iremos alimentar a função retrógrada correspondente). Para usá-lo, basta chamar:\n",
        "`` `python\n",
        "A, cache_de_ativação = sigmóide (Z)\n",
        "`` `\n",
        "\n",
        "- ** ReLU **: A fórmula matemática para ReLu é $ A = RELU (Z) = max (0, Z) $`. Esta função retorna ** dois ** itens: o valor de ativação \"` A` \"e um\" `cache`\" que contém \"` Z` \"(é o que você irá alimentar para a função de retrocesso correspondente). Para usá-lo, basta chamar:\n",
        "\n",
        "```python\n",
        "A, cache_de_ativação = relu (Z)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxkS0LGYLSvi"
      },
      "source": [
        "Para maior comodidade, iremos agrupar duas funções (Linear e Ativação) em uma função (LINEAR-> ATIVAÇÃO). Portanto, implementaremos uma função que faz o passo de avanço LINEAR, seguido por um passo de avanço de ATIVAÇÃO.\n",
        "\n",
        "<a name='ex-4'></a>\n",
        "### Demonstração 4 - linear_activation_forward\n",
        "\n",
        "Implementaremos a propagação direta da camada * LINEAR-> ATIVAÇÃO *. A relação matemática é: $ A ^ {[l]} = g (Z ^ {[l]}) = g (W ^ {[l]} A ^ {[l-1]} + b ^ {[l]} ) $ onde a ativação \"g\" pode ser sigmóide () ou relu (). Usaremos `linear_forward ()` e a função de ativação correta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjVNMig1Evxw"
      },
      "source": [
        "def linear_activation_forward (A_prev, W, b, activation):\n",
        "    \"\"\"\n",
        "    Implementar a propagação direta para a camada LINEAR-> ATIVAÇÃO\n",
        "\n",
        "    Argumentos:\n",
        "    A_prev - ativações da camada anterior (ou dados de entrada): (tamanho da camada anterior, número de exemplos)\n",
        "    W - matriz de pesos: matriz numpy de forma (tamanho da camada atual, tamanho da camada anterior)\n",
        "    b - vetor de polarização, matriz numpy de forma (tamanho da camada atual, 1)\n",
        "    activation - a ativação a ser usada nesta camada, armazenada como uma string de texto: \"sigmóide\" ou \"relu\"\n",
        "\n",
        "    Retorna:\n",
        "    A - a saída da função de ativação, também chamada de valor pós-ativação\n",
        "    cache - uma tupla python contendo \"linear_cache\" e \"activation_cache\";\n",
        "             armazenado para calcular o passe para trás de forma eficiente\n",
        "    \"\"\"\n",
        "    \n",
        "    if activation == \"sigmoid\":\n",
        "        Z, linear_cache = linear_forward (A_prev, W, b)\n",
        "        A, activation_cache = sigmoid (Z)\n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "        Z, linear_cache = linear_forward (A_prev, W, b)\n",
        "        A, activation_cache = relu (Z)\n",
        "\n",
        "    assert (A.shape == (W.shape [0], A_prev.shape [1]))\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kaby0pM1MOUK",
        "outputId": "0b908d6e-4cdb-48d7-eb25-fe2b13391973"
      },
      "source": [
        "t_A_prev, t_W, t_b = linear_activation_forward_test_case()\n",
        "\n",
        "t_A, t_linear_activation_cache = linear_activation_forward(t_A_prev, t_W, t_b, activation = \"sigmoid\")\n",
        "print(\"With sigmoid: A = \" + str(t_A))\n",
        "\n",
        "t_A, t_linear_activation_cache = linear_activation_forward(t_A_prev, t_W, t_b, activation = \"relu\")\n",
        "print(\"With ReLU: A = \" + str(t_A))\n",
        "\n",
        "linear_activation_forward_test(linear_activation_forward)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With sigmoid: A = [[0.96890023 0.11013289]]\n",
            "With ReLU: A = [[3.43896131 0.        ]]\n",
            "\u001b[92m All tests passed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvv5F9eyMQDH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}