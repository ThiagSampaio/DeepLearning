{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regressão Logística com Rede Neural.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1nveDQyK5q5x5dTkU1qXitWSmqw2JkMh4",
      "authorship_tag": "ABX9TyMmK39sKXdzNv4EdCyf3RKz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThiagSampaio/DeepLearning/blob/main/Regress%C3%A3o_Log%C3%ADstica_com_Rede_Neural.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQKRo2Q64aOJ"
      },
      "source": [
        "# Regressão Logística com Rede Neural\n",
        "\n",
        "Vamos construir um classificador de regressão logística para reconhecer gatos.\n",
        "\n",
        "** O que será mostrado: **\n",
        "- Construir uma arquitetura geral de um algoritmo de aprendizado, incluindo:\n",
        "    - Inicializando os parâmetros\n",
        "    - Calculando a função custo e seu gradiente\n",
        "    - Usando um algoritmo de otimização(gradiente descente)\n",
        "- Pegar as três funções acima e aplicá-las em uma função modelo principal, na ordem correta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKoeZy4M57yT"
      },
      "source": [
        "## Tabela de Conteúdos\n",
        "- [1 - Pacotes](#1)\n",
        "- [2 - Olhando o problema](#2)\n",
        "    - [Demonstração 1](#ex-1)\n",
        "    - [Demonstração 2](#ex-2)\n",
        "- [3 - Arquitetura geral de um algoritmo de aprendizado](#3)\n",
        "- [4 - Construindo as partes do nosso algoritmo](#4)\n",
        "    - [4.1 - Funções úteis](#4-1)\n",
        "        - [Demonstração 3 - sigmoid](#ex-3)\n",
        "    - [4.2 - Inicializando parâmetros](#4-2)\n",
        "        - [Demonstração 4 - inicializando_com_zeros](#ex-4)\n",
        "    - [4.3 - Propagação direta e reversa](#4-3)\n",
        "        - [Demonstração 5 - propagação](#ex-5)\n",
        "    - [4.4 - Optimization](#4-4)\n",
        "        - [Demonstração 6 - otimização](#ex-6)\n",
        "        - [Demonstração 7 - predição](#ex-7)\n",
        "- [5 - Aglutinar todas as funções em um modelo](#5)\n",
        "    - [Demonstração 8 - modelo](#ex-8)\n",
        "- [6 - Análises](#6)\n",
        "- [7 - Testando com sua prória imagem](#7)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-btR47T-fE1"
      },
      "source": [
        "<a name='1'></a>\n",
        "## 1 - Pacotes ##\n",
        "\n",
        "Primeiro, vamos rodar a célula abaixo para importar todos os pacotes que usaremos durante essa demonstração.\n",
        "- [numpy](https://numpy.org/doc/1.20/) é um pacote fundamental para computação científica com python\n",
        "- [h5py](http://www.h5py.org) é um pacote comum para interagir com arquivos armazenados como .h5\n",
        "- [matplotlib](http://matplotlib.org) é uma famosa biblioteca para plotar gráficos.\n",
        "- [PIL](https://pillow.readthedocs.io/en/stable/) and [scipy](https://www.scipy.org/) são usados para testar o modelo com sua própria figura no final do estudo. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU_MD31z4ZxX"
      },
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import scipy\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "from lr_utils  import load_dataset\n",
        "from public_tests import *\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDJgZdJPAuoQ"
      },
      "source": [
        "<a name='2'></a>\n",
        "## 2 - Olhando para o problema##\n",
        "\n",
        "**Declaração do Problema**: Foi dado a você um dataset(\"data.h5\") contendo:\n",
        "\n",
        "    - um conjunto de treinamento de imagens m_train rotuladas como gato (y = 1) ou não gato (y = 0)\n",
        "    - um conjunto de teste de imagens m_test rotuladas como gato ou não gato\n",
        "    - cada imagem tem forma (num_px, num_px, 3) onde 3 é para os 3 canais (RGB). Assim, cada imagem é quadrada (altura = num_px) e (largura = num_px).\n",
        "\n",
        "Construiremos um algoritmo de reconhecimento de imagem simples que pode classificar corretamente as imagens como gatos ou não.\n",
        "\n",
        "Vamos nos familiarizar mais com o conjunto de dados. Carregue os dados executando o código a seguir."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrOUfrJSCAnO"
      },
      "source": [
        "# Carregando os dados(gatos/não-gatos)\n",
        "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eee3At8gCki_"
      },
      "source": [
        "Adicionamos \"_orig\" no final dos dados de imagem (treino e teste) porque iremos fazer uma pré-processamento. Depois de processarm nós vamos acabar tendo train_set_x e test_set_x\n",
        "\n",
        "Cada linha do nosso train_set_x_orig and test_set_x_orig é uma array representando a imagem. Podemos visualizar um exemplo rodando o código abaixo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "SGQj8w_mCj_P",
        "outputId": "80fde330-c174-4cd0-fd71-a3cdc859d9e7"
      },
      "source": [
        "#Exemplo de imagem\n",
        "index = 50\n",
        "plt.imshow(train_set_x_orig[index])\n",
        "print (\"y = \" + str(train_set_y[:, index]) + \", é um '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' na foto.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y = [1], é um 'cat' na foto.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbvIa6SpNURz"
      },
      "source": [
        "Muitos bugs em aprendizagem profunda vem de ter matrizes/vetores com dimensões que não combinam. Se você puder manter suas matrizes/vetores com dimensões corretas, teremos um longo caminho sem bugs.\n",
        "\n",
        "<a name='ex-1'></a>\n",
        "### Demonstração 1\n",
        "Vamos achar os valores para:\n",
        "\n",
        "    - m_train (número de exemplos para o treino)\n",
        "    - m_test (número de exemplos para o teste)\n",
        "    - num_px (= altura = largura das imagens de treino)\n",
        "\n",
        "Lembre-se que `train_set_x_orig` é uma array numpy com o shape (m_train, num_px, num_px, 3).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhwzm0j5ChcS",
        "outputId": "c9c1e1af-001a-4da9-fc7b-d920c3fd27a8"
      },
      "source": [
        "m_train = train_set_x_orig.shape[0]\n",
        "m_test = test_set_x_orig.shape[0]\n",
        "num_px = train_set_x_orig.shape[1]\n",
        "\n",
        "print (\"Numero de exemplos para o treinamento: m_train = \" + str(m_train))\n",
        "print (\"Numero de exemplos para o teste: m_test = \" + str(m_test))\n",
        "print (\"Altura/Largura de cada image: num_px = \" + str(num_px))\n",
        "print (\"Cada imagem com o tamanho: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
        "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
        "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
        "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
        "print (\"test_set_y shape: \" + str(test_set_y.shape))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numero de exemplos para o treinamento: m_train = 209\n",
            "Numero de exemplos para o teste: m_test = 50\n",
            "Altura/Largura de cada image: num_px = 64\n",
            "Cada imagem com o tamanho: (64, 64, 3)\n",
            "train_set_x shape: (209, 64, 64, 3)\n",
            "train_set_y shape: (1, 209)\n",
            "test_set_x shape: (50, 64, 64, 3)\n",
            "test_set_y shape: (1, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V6th4zUX9U3"
      },
      "source": [
        "Remodelaremos os conjuntos de dados de treinamento e teste para que as imagens de tamanho (num_px, num_px, 3) sejam achatadas em vetores únicos de forma (num_px ∗ num_px ∗ 3, 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRUtHY4DX9Ga",
        "outputId": "c2ed42f6-ba5c-4225-dc03-fb47244c28b3"
      },
      "source": [
        "#Remodelando os dados de treino e teste\n",
        "\n",
        "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).T\n",
        "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0],-1).T\n",
        "\n",
        "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
        "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
        "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
        "print (\"test_set_y shape: \" + str(test_set_y.shape))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_set_x_flatten shape: (12288, 209)\n",
            "train_set_y shape: (1, 209)\n",
            "test_set_x_flatten shape: (12288, 50)\n",
            "test_set_y shape: (1, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6WASx4XcGy4"
      },
      "source": [
        "Para representar imagens coloridas(RGB), deve ser especificado para cada pixel os valores de cada canal, então cada pixel é na verdade um vetor de três números indo de 0 à 255.\n",
        "\n",
        "Um passo de pré-processamento em machine learning é centrar e padronizar nosso conjunto de dados, significando que você subtraí a média de toda array numpy de cada exemplo e depois divide cada exemplo pelo desvio da média padrão de toda numpy array. Porém para um dataset de imagem, é mais simples e mais conveniente e quase sempre entrega um bom resultado simplesmente dividir cada linha do dataset por 255.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HK4WRBYPdh0O"
      },
      "source": [
        "train_set_x = train_set_x_flatten / 255.\n",
        "test_set_x = test_set_x_flatten / 255."
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znkof7c8dvqK"
      },
      "source": [
        "<font color='blue'>\n",
        "    \n",
        "    \n",
        "**O que precisa ser lembrado:**\n",
        "\n",
        "Passos comuns para pré-processamento de um novo conjunto de dados:\n",
        "- Descobrir as dimensões e formas do problema (m_train, m_test, num_px, ...)\n",
        "- Remodele os conjuntos de dados de forma que cada exemplo seja agora um vetor de tamanho (num_px \\ * num_px \\ * 3, 1)\n",
        "- \"Padronizar\" os dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro5arVolkYAi"
      },
      "source": [
        "<a name='3'></a>\n",
        "## 3 - Arquitetura geral de um algoritmo de aprendizado ##\n",
        "\n",
        "É hora de projetar um algoritmo simples para distinguir imagens de gatos de imagens que não sejam de gatos.\n",
        "\n",
        "Você vai construir uma regressão logística, usando uma mentalidade de rede neural. A figura a seguir explica por que ** Regressão logística é na verdade uma rede neural muito simples! **\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1I_VwzE_JP4-lVfabHhyoh1DDUmjsfn9G)\n",
        "\n",
        "** Expressão matemática do algoritmo **:\n",
        "\n",
        "\n",
        "Por um exemplo $x^{(i)}$:\n",
        "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
        "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
        "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
        "\n",
        "O custo é então calculado somando todos os exemplos de treinamento:\n",
        "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n",
        "\n",
        "** Principais etapas **:\n",
        "Neste exercício, você executará as seguintes etapas:\n",
        "\n",
        "    - Inicialize os parâmetros do modelo\n",
        "    - Aprenda os parâmetros do modelo, minimizando o custo\n",
        "    - Use os parâmetros aprendidos para fazer previsões (no conjunto de teste)\n",
        "    - Analisar os resultados e concluir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPFyUnyGcYpo"
      },
      "source": [
        "<a name='4'></a>\n",
        "## 4 - Construindo as partes do nosso algoritmo ## \n",
        "\n",
        "Os principais passos para construir uma Rede Neural são:\n",
        "1. Definir a estrutura do modelo (tanto o número de variáveis de entrada\n",
        "2. Inicializar os parâmetros do modelo\n",
        "3. Loop:\n",
        "    - Calcular a perda recorrente (propagação direta)\n",
        "    - Calcular o gradiente (propagação inversa)\n",
        "    - Atualizar os parametros (gradiente descente)\n",
        "\n",
        "Normalmente iremos construir 1-3 separadamente e integrar tudo em umna função que chamaremos `model()`.\n",
        "\n",
        "<a name='4-1'></a>\n",
        "### 4.1 - Funções úteis \n",
        "\n",
        "<a name='ex-3'></a>\n",
        "### Demonstração 3 - sigmoid\n",
        "Vamos implementar uma função chamada `sigmoid()`. Como mostrado na figura acima, precisamos computar $sigmoid(z) = \\frac{1}{1 + e^{-z}}$ para $z = w^T x + b$ para fazer previsões."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7HaY0XVkVE_"
      },
      "source": [
        "# Função de classificação: sigmoid\n",
        "\n",
        "def sigmoid(z):\n",
        "  \"\"\"\n",
        "  Calcular o sigmoid de z\n",
        "\n",
        "  Argumentos:\n",
        "  z -- Um número ou uma numpy array de qualquer tamanho\n",
        "\n",
        "  Retorna:\n",
        "  s -- sigmoid(z)\n",
        "  \"\"\"\n",
        "  s = 1/(1 + np.exp(-z))\n",
        "\n",
        "  return s"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Fbpvxkxk9ww",
        "outputId": "96a0b5c1-a5ee-40d1-b7d5-3db9c60d61a1"
      },
      "source": [
        "# Teste_1 da função criada\n",
        "\n",
        "print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))\n",
        "\n",
        "sigmoid_test(sigmoid)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sigmoid([0, 2]) = [0.5        0.88079708]\n",
            "\u001b[92mAll tests passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_r4lmBik_02",
        "outputId": "8f76c539-5d4d-4212-f6ee-d2f6450c61ce"
      },
      "source": [
        "# Teste_2 da função criada\n",
        "\n",
        "x = np.array([0.5, 0, 2.0])\n",
        "output = sigmoid(x)\n",
        "print(output)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.62245933 0.5        0.88079708]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCkihBnnlU-z"
      },
      "source": [
        "<a name='4-2'></a>\n",
        "### 4.2 - Inicializando parâmetros\n",
        "\n",
        "<a name='ex-4'></a>\n",
        "### Demonstração 4 - inicializando com zeros \n",
        "Vamos implementar uma função de inicialização dos parâmetros W e b, fazendo com que eles se inicializem com valores = 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlLZHOxqlQXn"
      },
      "source": [
        "def initialize_with_zeros(dim):\n",
        "  \"\"\"\n",
        "  Esta função cria um vetor de zeros com o shape (dim,1) para W e inicializa b com 0\n",
        "\n",
        "  Argumento:\n",
        "  dim -- tamanho do vetor w que queremos(ou número de parametros neste caso)\n",
        "\n",
        "  Retorna:\n",
        "  w -- inicializado como um vetor de tamando (dim,1)\n",
        "  b -- Escalar inicial (correspondente ao viés) do tipo float\n",
        "  \"\"\"\n",
        "  w = np.zeros((dim,1))\n",
        "  b = 0.0\n",
        "\n",
        "  return w, b"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXeUClscnChQ",
        "outputId": "7cf788d7-beeb-4643-e2b8-f1eece6047d6"
      },
      "source": [
        "# Teste\n",
        "\n",
        "dim = 2\n",
        "w, b = initialize_with_zeros(dim)\n",
        "\n",
        "assert type(b) == float\n",
        "print (\"w = \" + str(w))\n",
        "print (\"b = \" + str(b))\n",
        "\n",
        "initialize_with_zeros_test(initialize_with_zeros)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w = [[0.]\n",
            " [0.]]\n",
            "b = 0.0\n",
            "\u001b[92mAll tests passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP17_GyTnVjG"
      },
      "source": [
        "<a name='4-3'></a>\n",
        "### 4.3 - Propagação direta e reversa\n",
        "\n",
        "Agora que nossos parâmetros estão inicializados, podemos fazer os passos da propagação direta e reversa para os parâmentros de aprendizado\n",
        "\n",
        "<a name='ex-5'></a>\n",
        "### Demonstação 5 - Propagação\n",
        "Vamos implementar a função `propagate()` que computa a função custo e o seu gradiente.\n",
        "\n",
        "**Fórmulas**:\n",
        "\n",
        "Propagação Direta:\n",
        "- Pegamos X\n",
        "- Computamos $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
        "- Calculamos o custo: $J = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)}))$\n",
        "\n",
        "Propagação Inversa:\n",
        "\n",
        "As fórmulas que iremos utilizar:\n",
        "\n",
        "\n",
        "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
        "\n",
        "\n",
        "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y43EjosBnHQW"
      },
      "source": [
        "def propagate(w, b, X, Y):\n",
        "      \"\"\"\n",
        "      Implementação da função custo e seu gradiente para a propagção explicada acima.\n",
        "\n",
        "      Argumentos:\n",
        "      w -- pesos, uma array numpy de tamanho (num_px * num_px * 3, 1)\n",
        "      b -- viés, um escalar. \n",
        "      X -- dados de tamanho (num_px * num_px * 3, numero de exemplos)\n",
        "      Y -- Vetor de respostas (contendo 0 se não for gato e 1 se for) de tamanho (1, numero de exemplos)\n",
        "\n",
        "      retorna:\n",
        "      \"\"\"\n",
        "      m = X.shape[1]\n",
        "\n",
        "      # Propagação direta (De X para o Custo)\n",
        "\n",
        "      A = sigmoid(np.dot(w.T,X) + b)\n",
        "      cost = (-1/m)*(np.dot(Y,np.log(A).T) + np.dot((1-Y),np.log(1-A).T))\n",
        "\n",
        "      # Propagação Reversa (Achar o gradiente)\n",
        "\n",
        "      dw = 1 / m *(np.dot(X, (A-Y).T))\n",
        "      db = 1 / m *(np.sum(A - Y))\n",
        "\n",
        "      assert(dw.shape == w.shape)\n",
        "      assert(db.dtype == float)\n",
        "    \n",
        "      cost = np.squeeze(np.array(cost))\n",
        "      assert(cost.shape == ())\n",
        "    \n",
        "      grads = {\"dw\": dw,\n",
        "              \"db\": db}\n",
        "    \n",
        "      return grads, cost\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOuuC_PyqYO_",
        "outputId": "18810f83-47ef-40ec-f52f-3a98ef59fc80"
      },
      "source": [
        "# Teste da função \n",
        "w =  np.array([[1.], [2.]])\n",
        "b = 2.\n",
        "X =np.array([[1., 2., -1.], [3., 4., -3.2]])\n",
        "Y = np.array([[1, 0, 1]])\n",
        "grads, cost = propagate(w, b, X, Y)\n",
        "\n",
        "assert type(grads[\"dw\"]) == np.ndarray\n",
        "assert grads[\"dw\"].shape == (2, 1)\n",
        "assert type(grads[\"db\"]) == np.float64\n",
        "\n",
        "\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))\n",
        "print (\"cost = \" + str(cost))\n",
        "\n",
        "propagate_test(propagate)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dw = [[0.99845601]\n",
            " [2.39507239]]\n",
            "db = 0.001455578136784208\n",
            "cost = 5.801545319394553\n",
            "\u001b[92mAll tests passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biuddX5T4F7s"
      },
      "source": [
        "<a name='4-4'></a>\n",
        "### 4.4 - Otimização\n",
        "-  Inicializamos os parâmetros.\n",
        "-  Computamos a função custo e seu gradiente \n",
        "-  Precisamos fazer o update dos parâmetros usando gradiente descente\n",
        "\n",
        "### Demonstração 6 - Otimização\n",
        "Vamos escrever a função de otimização. O objetivo é aprender $w$ e $b$ para minimizar a função custo $J$. Para o parâmetro $\\theta$, a fórmula de atualização é $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, onde $\\alpha$ é a taxa de aprendizagem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tAr-jJ14FJc"
      },
      "source": [
        "def optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):\n",
        "  \"\"\"\n",
        "  Esta função otimiza w e b rodando o algoritmo gradiente descendente\n",
        "\n",
        "  Argumentos:\n",
        "  w -- pesos, uma array numpy de tamanho(num_px*num_px*3, 1)\n",
        "  b -- viés,um escalar\n",
        "  X -- Dados de formato (num_px * num_px * 3, 1)\n",
        "  Y -- Vetor de respostas de formato(1, numero de exemplos)\n",
        "  num_iterations -- numero de iterações do loop de otimização\n",
        "  learning_rate -- taxa de aprendizado do gradiente descendente\n",
        "  print_cost -- Verdadeiro para printar a perda a cada 100 passos\n",
        "\n",
        "  Retorna:\n",
        "  params -- Dicionario contendo os pesos w e os vieses b\n",
        "  grads -- Dicionario contendo os gradientes dos pesos e vieses com sua respectiva função custo\n",
        "  costs -- lista de todos os custos computados durante a otimização, será usado para plotar o gráfico de aprendizado\n",
        "  \"\"\"\n",
        "  w = copy.deepcopy(w)\n",
        "  b = copy.deepcopy(b)\n",
        "\n",
        "  costs = []\n",
        "\n",
        "  for i in range(num_iterations):\n",
        "    \n",
        "    #  Calculo do custo e gradiente\n",
        "    grads,cost = propagate(w, b, X, Y)\n",
        "\n",
        "    # Pegando os valores de grads\n",
        "    dw = grads[\"dw\"]\n",
        "    db = grads[\"db\"]\n",
        "\n",
        "    #Regra de update\n",
        "    w = w - learning_rate*dw\n",
        "    b = b - learning_rate*db\n",
        "\n",
        "    #Guardando dados\n",
        "    if i % 100 == 0:\n",
        "      costs.append(cost)\n",
        "\n",
        "      if print_cost:\n",
        "        print(\"Custo depois da iteração %i: %f\" %(i, cost))\n",
        "\n",
        "  params = {\"w\":w,\n",
        "            \"b\":b}\n",
        "\n",
        "  grads = {\"dw\":dw,\n",
        "            \"db\":db}\n",
        "  return params, grads, costs\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1Hrr2bL0BOL",
        "outputId": "7c107cf4-a297-4c71-cf68-6f80516b57ef"
      },
      "source": [
        "params, grads, costs = optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False)\n",
        "\n",
        "print (\"w = \" + str(params[\"w\"]))\n",
        "print (\"b = \" + str(params[\"b\"]))\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))\n",
        "print(\"Costs = \" + str(costs))\n",
        "\n",
        "optimize_test(optimize)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w = [[0.19033591]\n",
            " [0.12259159]]\n",
            "b = 1.9253598300845747\n",
            "dw = [[0.67752042]\n",
            " [1.41625495]]\n",
            "db = 0.21919450454067652\n",
            "Costs = [array(5.80154532)]\n",
            "\u001b[92mAll tests passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EemtpvFuMRiv"
      },
      "source": [
        "<a name='ex-7'></a>\n",
        "### Exercise 7 - predição\n",
        "A função anterior irá ter como o output w e b. \n",
        "Nós podemos usar w e b para predizer as respostar para o dataset X. Vamos implementar a função `predict()`. Para isso devemos seguir os seguintes passos:\n",
        "\n",
        "1. Calcular $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
        "\n",
        "2. Converter as entradas em 0 (se a ativação <= 0.5) ou 1( se ativação > 0.5), guardar as predições em `Y_prediction`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNhciMRe0FuT"
      },
      "source": [
        "def predict(w, b, X):\n",
        "  \"\"\"\n",
        "  Predizer quando a resposta for 0 ou 1 usando regressão logistica\n",
        "  \n",
        "  Argumentos:\n",
        "  w -- pesos, uma array numpy de tamanho(num_px*num_px*3, 1)\n",
        "  b -- viés, escalar.\n",
        "  X -- Dados de formato (num_px * num_px * 3, 1)\n",
        "\n",
        "  Retorna:\n",
        "\n",
        "  Y_prediction -- uma array numpy (vetor) contendo todas predições (0/1) para os exemplos em X\n",
        "  \"\"\"\n",
        "  m = X.shape[1]\n",
        "  Y_prediction = np.zeros((1, m))\n",
        "  w = w.reshape(X.shape[0],1)\n",
        "\n",
        "  #Computar o vetor A prevendo a probabilidade de ser um gato na figura\n",
        "  A = sigmoid(np.dot(w.T,X) + b)\n",
        "\n",
        "  for i in range(A.shape[1]):\n",
        "        \n",
        "    if (A[0][i] <= 0.5):\n",
        "      Y_prediction[0][i] = 0\n",
        "    else:\n",
        "      Y_prediction[0][i] = 1\n",
        "      \n",
        "  return Y_prediction       "
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04p-Zrn00QL0",
        "outputId": "e94896e3-0485-4177-a79b-d1553fd56137"
      },
      "source": [
        "w = np.array([[0.1124579], [0.23106775]])\n",
        "b = -0.3\n",
        "X = np.array([[1., -1.1, -3.2],[1.2, 2., 0.1]])\n",
        "print (\"predictions = \" + str(predict(w, b, X)))\n",
        "\n",
        "predict_test(predict)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predictions = [[1. 1. 0.]]\n",
            "\u001b[92mAll tests passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6tAlHpvO8GX"
      },
      "source": [
        "# GRADED FUNCTION: model\n",
        "\n",
        "def model(X_train, Y_train, X_test, Y_test, num_iterations=10000, learning_rate=0.5, print_cost=False):\n",
        "    \"\"\"\n",
        "    Builds the logistic regression model by calling the function you've implemented previously\n",
        "    \n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
        "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
        "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
        "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
        "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_cost -- Set to True to print the cost every 100 iterations\n",
        "    \n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "    # (≈ 1 line of code)   \n",
        "    # initialize parameters with zeros \n",
        "    # w, b = w, b = initialize_with_zeros(X_train.shape[0])\n",
        "    \n",
        "    #(≈ 1 line of code)\n",
        "    # Gradient descent \n",
        "    # params, grads, costs = ...\n",
        "    \n",
        "    # Retrieve parameters w and b from dictionary \"params\"\n",
        "    # w = ...\n",
        "    # b = ...\n",
        "    \n",
        "    # Predict test/train set examples (≈ 2 lines of code)\n",
        "    # Y_prediction_test = ...\n",
        "    # Y_prediction_train = ...\n",
        "    \n",
        "    # YOUR CODE STARTS HERE\n",
        "    w, b = initialize_with_zeros(X_train.shape[0])\n",
        "    \n",
        "    parameters, grads, costs =  optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
        "    \n",
        "    \n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "    \n",
        "    Y_prediction_test = predict(w, b, X_test)\n",
        "    Y_prediction_train = predict(w, b, X_train)\n",
        "    \n",
        "    # YOUR CODE ENDS HERE\n",
        "\n",
        "    # Print train/test Errors\n",
        "    if print_cost:\n",
        "        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "    \n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"w\" : w, \n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "    \n",
        "    return d"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huYxhA8ePS9-",
        "outputId": "a505ca38-a159-4e7b-ff00-96183cda56bd"
      },
      "source": [
        "model_test(model)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[92mAll tests passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQgCq5F_PVEO",
        "outputId": "c8e34642-d309-4059-d57e-da0e22179a4f"
      },
      "source": [
        "logistic_regression_model = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=50000, learning_rate=0.005, print_cost=True)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Custo depois da iteração 0: 0.693147\n",
            "Custo depois da iteração 100: 0.584508\n",
            "Custo depois da iteração 200: 0.466949\n",
            "Custo depois da iteração 300: 0.376007\n",
            "Custo depois da iteração 400: 0.331463\n",
            "Custo depois da iteração 500: 0.303273\n",
            "Custo depois da iteração 600: 0.279880\n",
            "Custo depois da iteração 700: 0.260042\n",
            "Custo depois da iteração 800: 0.242941\n",
            "Custo depois da iteração 900: 0.228004\n",
            "Custo depois da iteração 1000: 0.214820\n",
            "Custo depois da iteração 1100: 0.203078\n",
            "Custo depois da iteração 1200: 0.192544\n",
            "Custo depois da iteração 1300: 0.183033\n",
            "Custo depois da iteração 1400: 0.174399\n",
            "Custo depois da iteração 1500: 0.166521\n",
            "Custo depois da iteração 1600: 0.159305\n",
            "Custo depois da iteração 1700: 0.152667\n",
            "Custo depois da iteração 1800: 0.146542\n",
            "Custo depois da iteração 1900: 0.140872\n",
            "Custo depois da iteração 2000: 0.135608\n",
            "Custo depois da iteração 2100: 0.130708\n",
            "Custo depois da iteração 2200: 0.126137\n",
            "Custo depois da iteração 2300: 0.121861\n",
            "Custo depois da iteração 2400: 0.117855\n",
            "Custo depois da iteração 2500: 0.114093\n",
            "Custo depois da iteração 2600: 0.110554\n",
            "Custo depois da iteração 2700: 0.107219\n",
            "Custo depois da iteração 2800: 0.104072\n",
            "Custo depois da iteração 2900: 0.101097\n",
            "Custo depois da iteração 3000: 0.098280\n",
            "Custo depois da iteração 3100: 0.095610\n",
            "Custo depois da iteração 3200: 0.093075\n",
            "Custo depois da iteração 3300: 0.090667\n",
            "Custo depois da iteração 3400: 0.088374\n",
            "Custo depois da iteração 3500: 0.086190\n",
            "Custo depois da iteração 3600: 0.084108\n",
            "Custo depois da iteração 3700: 0.082119\n",
            "Custo depois da iteração 3800: 0.080219\n",
            "Custo depois da iteração 3900: 0.078402\n",
            "Custo depois da iteração 4000: 0.076662\n",
            "Custo depois da iteração 4100: 0.074994\n",
            "Custo depois da iteração 4200: 0.073395\n",
            "Custo depois da iteração 4300: 0.071860\n",
            "Custo depois da iteração 4400: 0.070385\n",
            "Custo depois da iteração 4500: 0.068968\n",
            "Custo depois da iteração 4600: 0.067604\n",
            "Custo depois da iteração 4700: 0.066291\n",
            "Custo depois da iteração 4800: 0.065027\n",
            "Custo depois da iteração 4900: 0.063807\n",
            "Custo depois da iteração 5000: 0.062631\n",
            "Custo depois da iteração 5100: 0.061496\n",
            "Custo depois da iteração 5200: 0.060400\n",
            "Custo depois da iteração 5300: 0.059341\n",
            "Custo depois da iteração 5400: 0.058317\n",
            "Custo depois da iteração 5500: 0.057327\n",
            "Custo depois da iteração 5600: 0.056368\n",
            "Custo depois da iteração 5700: 0.055440\n",
            "Custo depois da iteração 5800: 0.054541\n",
            "Custo depois da iteração 5900: 0.053669\n",
            "Custo depois da iteração 6000: 0.052824\n",
            "Custo depois da iteração 6100: 0.052005\n",
            "Custo depois da iteração 6200: 0.051209\n",
            "Custo depois da iteração 6300: 0.050436\n",
            "Custo depois da iteração 6400: 0.049686\n",
            "Custo depois da iteração 6500: 0.048957\n",
            "Custo depois da iteração 6600: 0.048248\n",
            "Custo depois da iteração 6700: 0.047559\n",
            "Custo depois da iteração 6800: 0.046888\n",
            "Custo depois da iteração 6900: 0.046236\n",
            "Custo depois da iteração 7000: 0.045601\n",
            "Custo depois da iteração 7100: 0.044982\n",
            "Custo depois da iteração 7200: 0.044380\n",
            "Custo depois da iteração 7300: 0.043793\n",
            "Custo depois da iteração 7400: 0.043220\n",
            "Custo depois da iteração 7500: 0.042662\n",
            "Custo depois da iteração 7600: 0.042118\n",
            "Custo depois da iteração 7700: 0.041587\n",
            "Custo depois da iteração 7800: 0.041069\n",
            "Custo depois da iteração 7900: 0.040563\n",
            "Custo depois da iteração 8000: 0.040069\n",
            "Custo depois da iteração 8100: 0.039587\n",
            "Custo depois da iteração 8200: 0.039116\n",
            "Custo depois da iteração 8300: 0.038655\n",
            "Custo depois da iteração 8400: 0.038205\n",
            "Custo depois da iteração 8500: 0.037765\n",
            "Custo depois da iteração 8600: 0.037335\n",
            "Custo depois da iteração 8700: 0.036914\n",
            "Custo depois da iteração 8800: 0.036502\n",
            "Custo depois da iteração 8900: 0.036099\n",
            "Custo depois da iteração 9000: 0.035704\n",
            "Custo depois da iteração 9100: 0.035318\n",
            "Custo depois da iteração 9200: 0.034940\n",
            "Custo depois da iteração 9300: 0.034570\n",
            "Custo depois da iteração 9400: 0.034207\n",
            "Custo depois da iteração 9500: 0.033851\n",
            "Custo depois da iteração 9600: 0.033503\n",
            "Custo depois da iteração 9700: 0.033161\n",
            "Custo depois da iteração 9800: 0.032826\n",
            "Custo depois da iteração 9900: 0.032498\n",
            "Custo depois da iteração 10000: 0.032176\n",
            "Custo depois da iteração 10100: 0.031860\n",
            "Custo depois da iteração 10200: 0.031550\n",
            "Custo depois da iteração 10300: 0.031246\n",
            "Custo depois da iteração 10400: 0.030948\n",
            "Custo depois da iteração 10500: 0.030655\n",
            "Custo depois da iteração 10600: 0.030367\n",
            "Custo depois da iteração 10700: 0.030085\n",
            "Custo depois da iteração 10800: 0.029808\n",
            "Custo depois da iteração 10900: 0.029535\n",
            "Custo depois da iteração 11000: 0.029268\n",
            "Custo depois da iteração 11100: 0.029005\n",
            "Custo depois da iteração 11200: 0.028747\n",
            "Custo depois da iteração 11300: 0.028493\n",
            "Custo depois da iteração 11400: 0.028243\n",
            "Custo depois da iteração 11500: 0.027998\n",
            "Custo depois da iteração 11600: 0.027757\n",
            "Custo depois da iteração 11700: 0.027520\n",
            "Custo depois da iteração 11800: 0.027287\n",
            "Custo depois da iteração 11900: 0.027057\n",
            "Custo depois da iteração 12000: 0.026831\n",
            "Custo depois da iteração 12100: 0.026609\n",
            "Custo depois da iteração 12200: 0.026391\n",
            "Custo depois da iteração 12300: 0.026176\n",
            "Custo depois da iteração 12400: 0.025964\n",
            "Custo depois da iteração 12500: 0.025756\n",
            "Custo depois da iteração 12600: 0.025551\n",
            "Custo depois da iteração 12700: 0.025349\n",
            "Custo depois da iteração 12800: 0.025150\n",
            "Custo depois da iteração 12900: 0.024955\n",
            "Custo depois da iteração 13000: 0.024762\n",
            "Custo depois da iteração 13100: 0.024572\n",
            "Custo depois da iteração 13200: 0.024385\n",
            "Custo depois da iteração 13300: 0.024200\n",
            "Custo depois da iteração 13400: 0.024019\n",
            "Custo depois da iteração 13500: 0.023840\n",
            "Custo depois da iteração 13600: 0.023664\n",
            "Custo depois da iteração 13700: 0.023490\n",
            "Custo depois da iteração 13800: 0.023318\n",
            "Custo depois da iteração 13900: 0.023149\n",
            "Custo depois da iteração 14000: 0.022983\n",
            "Custo depois da iteração 14100: 0.022819\n",
            "Custo depois da iteração 14200: 0.022657\n",
            "Custo depois da iteração 14300: 0.022497\n",
            "Custo depois da iteração 14400: 0.022339\n",
            "Custo depois da iteração 14500: 0.022184\n",
            "Custo depois da iteração 14600: 0.022031\n",
            "Custo depois da iteração 14700: 0.021880\n",
            "Custo depois da iteração 14800: 0.021730\n",
            "Custo depois da iteração 14900: 0.021583\n",
            "Custo depois da iteração 15000: 0.021438\n",
            "Custo depois da iteração 15100: 0.021295\n",
            "Custo depois da iteração 15200: 0.021153\n",
            "Custo depois da iteração 15300: 0.021014\n",
            "Custo depois da iteração 15400: 0.020876\n",
            "Custo depois da iteração 15500: 0.020740\n",
            "Custo depois da iteração 15600: 0.020605\n",
            "Custo depois da iteração 15700: 0.020473\n",
            "Custo depois da iteração 15800: 0.020342\n",
            "Custo depois da iteração 15900: 0.020212\n",
            "Custo depois da iteração 16000: 0.020084\n",
            "Custo depois da iteração 16100: 0.019958\n",
            "Custo depois da iteração 16200: 0.019834\n",
            "Custo depois da iteração 16300: 0.019711\n",
            "Custo depois da iteração 16400: 0.019589\n",
            "Custo depois da iteração 16500: 0.019469\n",
            "Custo depois da iteração 16600: 0.019350\n",
            "Custo depois da iteração 16700: 0.019233\n",
            "Custo depois da iteração 16800: 0.019117\n",
            "Custo depois da iteração 16900: 0.019002\n",
            "Custo depois da iteração 17000: 0.018889\n",
            "Custo depois da iteração 17100: 0.018777\n",
            "Custo depois da iteração 17200: 0.018667\n",
            "Custo depois da iteração 17300: 0.018557\n",
            "Custo depois da iteração 17400: 0.018449\n",
            "Custo depois da iteração 17500: 0.018342\n",
            "Custo depois da iteração 17600: 0.018237\n",
            "Custo depois da iteração 17700: 0.018132\n",
            "Custo depois da iteração 17800: 0.018029\n",
            "Custo depois da iteração 17900: 0.017927\n",
            "Custo depois da iteração 18000: 0.017826\n",
            "Custo depois da iteração 18100: 0.017726\n",
            "Custo depois da iteração 18200: 0.017627\n",
            "Custo depois da iteração 18300: 0.017530\n",
            "Custo depois da iteração 18400: 0.017433\n",
            "Custo depois da iteração 18500: 0.017337\n",
            "Custo depois da iteração 18600: 0.017243\n",
            "Custo depois da iteração 18700: 0.017149\n",
            "Custo depois da iteração 18800: 0.017057\n",
            "Custo depois da iteração 18900: 0.016965\n",
            "Custo depois da iteração 19000: 0.016874\n",
            "Custo depois da iteração 19100: 0.016785\n",
            "Custo depois da iteração 19200: 0.016696\n",
            "Custo depois da iteração 19300: 0.016608\n",
            "Custo depois da iteração 19400: 0.016521\n",
            "Custo depois da iteração 19500: 0.016435\n",
            "Custo depois da iteração 19600: 0.016350\n",
            "Custo depois da iteração 19700: 0.016266\n",
            "Custo depois da iteração 19800: 0.016182\n",
            "Custo depois da iteração 19900: 0.016100\n",
            "Custo depois da iteração 20000: 0.016018\n",
            "Custo depois da iteração 20100: 0.015937\n",
            "Custo depois da iteração 20200: 0.015857\n",
            "Custo depois da iteração 20300: 0.015777\n",
            "Custo depois da iteração 20400: 0.015699\n",
            "Custo depois da iteração 20500: 0.015621\n",
            "Custo depois da iteração 20600: 0.015544\n",
            "Custo depois da iteração 20700: 0.015468\n",
            "Custo depois da iteração 20800: 0.015392\n",
            "Custo depois da iteração 20900: 0.015317\n",
            "Custo depois da iteração 21000: 0.015243\n",
            "Custo depois da iteração 21100: 0.015170\n",
            "Custo depois da iteração 21200: 0.015097\n",
            "Custo depois da iteração 21300: 0.015025\n",
            "Custo depois da iteração 21400: 0.014953\n",
            "Custo depois da iteração 21500: 0.014883\n",
            "Custo depois da iteração 21600: 0.014813\n",
            "Custo depois da iteração 21700: 0.014743\n",
            "Custo depois da iteração 21800: 0.014674\n",
            "Custo depois da iteração 21900: 0.014606\n",
            "Custo depois da iteração 22000: 0.014539\n",
            "Custo depois da iteração 22100: 0.014472\n",
            "Custo depois da iteração 22200: 0.014406\n",
            "Custo depois da iteração 22300: 0.014340\n",
            "Custo depois da iteração 22400: 0.014275\n",
            "Custo depois da iteração 22500: 0.014210\n",
            "Custo depois da iteração 22600: 0.014146\n",
            "Custo depois da iteração 22700: 0.014083\n",
            "Custo depois da iteração 22800: 0.014020\n",
            "Custo depois da iteração 22900: 0.013958\n",
            "Custo depois da iteração 23000: 0.013896\n",
            "Custo depois da iteração 23100: 0.013835\n",
            "Custo depois da iteração 23200: 0.013774\n",
            "Custo depois da iteração 23300: 0.013714\n",
            "Custo depois da iteração 23400: 0.013654\n",
            "Custo depois da iteração 23500: 0.013595\n",
            "Custo depois da iteração 23600: 0.013537\n",
            "Custo depois da iteração 23700: 0.013478\n",
            "Custo depois da iteração 23800: 0.013421\n",
            "Custo depois da iteração 23900: 0.013364\n",
            "Custo depois da iteração 24000: 0.013307\n",
            "Custo depois da iteração 24100: 0.013251\n",
            "Custo depois da iteração 24200: 0.013195\n",
            "Custo depois da iteração 24300: 0.013140\n",
            "Custo depois da iteração 24400: 0.013085\n",
            "Custo depois da iteração 24500: 0.013031\n",
            "Custo depois da iteração 24600: 0.012977\n",
            "Custo depois da iteração 24700: 0.012923\n",
            "Custo depois da iteração 24800: 0.012870\n",
            "Custo depois da iteração 24900: 0.012818\n",
            "Custo depois da iteração 25000: 0.012765\n",
            "Custo depois da iteração 25100: 0.012714\n",
            "Custo depois da iteração 25200: 0.012662\n",
            "Custo depois da iteração 25300: 0.012611\n",
            "Custo depois da iteração 25400: 0.012561\n",
            "Custo depois da iteração 25500: 0.012511\n",
            "Custo depois da iteração 25600: 0.012461\n",
            "Custo depois da iteração 25700: 0.012411\n",
            "Custo depois da iteração 25800: 0.012362\n",
            "Custo depois da iteração 25900: 0.012314\n",
            "Custo depois da iteração 26000: 0.012266\n",
            "Custo depois da iteração 26100: 0.012218\n",
            "Custo depois da iteração 26200: 0.012170\n",
            "Custo depois da iteração 26300: 0.012123\n",
            "Custo depois da iteração 26400: 0.012076\n",
            "Custo depois da iteração 26500: 0.012030\n",
            "Custo depois da iteração 26600: 0.011984\n",
            "Custo depois da iteração 26700: 0.011938\n",
            "Custo depois da iteração 26800: 0.011893\n",
            "Custo depois da iteração 26900: 0.011848\n",
            "Custo depois da iteração 27000: 0.011803\n",
            "Custo depois da iteração 27100: 0.011759\n",
            "Custo depois da iteração 27200: 0.011715\n",
            "Custo depois da iteração 27300: 0.011671\n",
            "Custo depois da iteração 27400: 0.011628\n",
            "Custo depois da iteração 27500: 0.011585\n",
            "Custo depois da iteração 27600: 0.011542\n",
            "Custo depois da iteração 27700: 0.011499\n",
            "Custo depois da iteração 27800: 0.011457\n",
            "Custo depois da iteração 27900: 0.011415\n",
            "Custo depois da iteração 28000: 0.011374\n",
            "Custo depois da iteração 28100: 0.011333\n",
            "Custo depois da iteração 28200: 0.011292\n",
            "Custo depois da iteração 28300: 0.011251\n",
            "Custo depois da iteração 28400: 0.011211\n",
            "Custo depois da iteração 28500: 0.011171\n",
            "Custo depois da iteração 28600: 0.011131\n",
            "Custo depois da iteração 28700: 0.011091\n",
            "Custo depois da iteração 28800: 0.011052\n",
            "Custo depois da iteração 28900: 0.011013\n",
            "Custo depois da iteração 29000: 0.010975\n",
            "Custo depois da iteração 29100: 0.010936\n",
            "Custo depois da iteração 29200: 0.010898\n",
            "Custo depois da iteração 29300: 0.010860\n",
            "Custo depois da iteração 29400: 0.010822\n",
            "Custo depois da iteração 29500: 0.010785\n",
            "Custo depois da iteração 29600: 0.010748\n",
            "Custo depois da iteração 29700: 0.010711\n",
            "Custo depois da iteração 29800: 0.010674\n",
            "Custo depois da iteração 29900: 0.010638\n",
            "Custo depois da iteração 30000: 0.010602\n",
            "Custo depois da iteração 30100: 0.010566\n",
            "Custo depois da iteração 30200: 0.010530\n",
            "Custo depois da iteração 30300: 0.010495\n",
            "Custo depois da iteração 30400: 0.010460\n",
            "Custo depois da iteração 30500: 0.010425\n",
            "Custo depois da iteração 30600: 0.010390\n",
            "Custo depois da iteração 30700: 0.010356\n",
            "Custo depois da iteração 30800: 0.010322\n",
            "Custo depois da iteração 30900: 0.010288\n",
            "Custo depois da iteração 31000: 0.010254\n",
            "Custo depois da iteração 31100: 0.010220\n",
            "Custo depois da iteração 31200: 0.010187\n",
            "Custo depois da iteração 31300: 0.010154\n",
            "Custo depois da iteração 31400: 0.010121\n",
            "Custo depois da iteração 31500: 0.010088\n",
            "Custo depois da iteração 31600: 0.010055\n",
            "Custo depois da iteração 31700: 0.010023\n",
            "Custo depois da iteração 31800: 0.009991\n",
            "Custo depois da iteração 31900: 0.009959\n",
            "Custo depois da iteração 32000: 0.009927\n",
            "Custo depois da iteração 32100: 0.009896\n",
            "Custo depois da iteração 32200: 0.009865\n",
            "Custo depois da iteração 32300: 0.009833\n",
            "Custo depois da iteração 32400: 0.009803\n",
            "Custo depois da iteração 32500: 0.009772\n",
            "Custo depois da iteração 32600: 0.009741\n",
            "Custo depois da iteração 32700: 0.009711\n",
            "Custo depois da iteração 32800: 0.009681\n",
            "Custo depois da iteração 32900: 0.009651\n",
            "Custo depois da iteração 33000: 0.009621\n",
            "Custo depois da iteração 33100: 0.009591\n",
            "Custo depois da iteração 33200: 0.009562\n",
            "Custo depois da iteração 33300: 0.009533\n",
            "Custo depois da iteração 33400: 0.009504\n",
            "Custo depois da iteração 33500: 0.009475\n",
            "Custo depois da iteração 33600: 0.009446\n",
            "Custo depois da iteração 33700: 0.009417\n",
            "Custo depois da iteração 33800: 0.009389\n",
            "Custo depois da iteração 33900: 0.009361\n",
            "Custo depois da iteração 34000: 0.009333\n",
            "Custo depois da iteração 34100: 0.009305\n",
            "Custo depois da iteração 34200: 0.009277\n",
            "Custo depois da iteração 34300: 0.009250\n",
            "Custo depois da iteração 34400: 0.009222\n",
            "Custo depois da iteração 34500: 0.009195\n",
            "Custo depois da iteração 34600: 0.009168\n",
            "Custo depois da iteração 34700: 0.009141\n",
            "Custo depois da iteração 34800: 0.009114\n",
            "Custo depois da iteração 34900: 0.009088\n",
            "Custo depois da iteração 35000: 0.009061\n",
            "Custo depois da iteração 35100: 0.009035\n",
            "Custo depois da iteração 35200: 0.009009\n",
            "Custo depois da iteração 35300: 0.008983\n",
            "Custo depois da iteração 35400: 0.008957\n",
            "Custo depois da iteração 35500: 0.008931\n",
            "Custo depois da iteração 35600: 0.008906\n",
            "Custo depois da iteração 35700: 0.008880\n",
            "Custo depois da iteração 35800: 0.008855\n",
            "Custo depois da iteração 35900: 0.008830\n",
            "Custo depois da iteração 36000: 0.008805\n",
            "Custo depois da iteração 36100: 0.008780\n",
            "Custo depois da iteração 36200: 0.008755\n",
            "Custo depois da iteração 36300: 0.008731\n",
            "Custo depois da iteração 36400: 0.008706\n",
            "Custo depois da iteração 36500: 0.008682\n",
            "Custo depois da iteração 36600: 0.008658\n",
            "Custo depois da iteração 36700: 0.008634\n",
            "Custo depois da iteração 36800: 0.008610\n",
            "Custo depois da iteração 36900: 0.008586\n",
            "Custo depois da iteração 37000: 0.008563\n",
            "Custo depois da iteração 37100: 0.008539\n",
            "Custo depois da iteração 37200: 0.008516\n",
            "Custo depois da iteração 37300: 0.008492\n",
            "Custo depois da iteração 37400: 0.008469\n",
            "Custo depois da iteração 37500: 0.008446\n",
            "Custo depois da iteração 37600: 0.008423\n",
            "Custo depois da iteração 37700: 0.008401\n",
            "Custo depois da iteração 37800: 0.008378\n",
            "Custo depois da iteração 37900: 0.008356\n",
            "Custo depois da iteração 38000: 0.008333\n",
            "Custo depois da iteração 38100: 0.008311\n",
            "Custo depois da iteração 38200: 0.008289\n",
            "Custo depois da iteração 38300: 0.008267\n",
            "Custo depois da iteração 38400: 0.008245\n",
            "Custo depois da iteração 38500: 0.008223\n",
            "Custo depois da iteração 38600: 0.008201\n",
            "Custo depois da iteração 38700: 0.008180\n",
            "Custo depois da iteração 38800: 0.008158\n",
            "Custo depois da iteração 38900: 0.008137\n",
            "Custo depois da iteração 39000: 0.008116\n",
            "Custo depois da iteração 39100: 0.008094\n",
            "Custo depois da iteração 39200: 0.008073\n",
            "Custo depois da iteração 39300: 0.008053\n",
            "Custo depois da iteração 39400: 0.008032\n",
            "Custo depois da iteração 39500: 0.008011\n",
            "Custo depois da iteração 39600: 0.007990\n",
            "Custo depois da iteração 39700: 0.007970\n",
            "Custo depois da iteração 39800: 0.007950\n",
            "Custo depois da iteração 39900: 0.007929\n",
            "Custo depois da iteração 40000: 0.007909\n",
            "Custo depois da iteração 40100: 0.007889\n",
            "Custo depois da iteração 40200: 0.007869\n",
            "Custo depois da iteração 40300: 0.007849\n",
            "Custo depois da iteração 40400: 0.007829\n",
            "Custo depois da iteração 40500: 0.007810\n",
            "Custo depois da iteração 40600: 0.007790\n",
            "Custo depois da iteração 40700: 0.007771\n",
            "Custo depois da iteração 40800: 0.007751\n",
            "Custo depois da iteração 40900: 0.007732\n",
            "Custo depois da iteração 41000: 0.007713\n",
            "Custo depois da iteração 41100: 0.007694\n",
            "Custo depois da iteração 41200: 0.007675\n",
            "Custo depois da iteração 41300: 0.007656\n",
            "Custo depois da iteração 41400: 0.007637\n",
            "Custo depois da iteração 41500: 0.007618\n",
            "Custo depois da iteração 41600: 0.007599\n",
            "Custo depois da iteração 41700: 0.007581\n",
            "Custo depois da iteração 41800: 0.007562\n",
            "Custo depois da iteração 41900: 0.007544\n",
            "Custo depois da iteração 42000: 0.007526\n",
            "Custo depois da iteração 42100: 0.007508\n",
            "Custo depois da iteração 42200: 0.007489\n",
            "Custo depois da iteração 42300: 0.007471\n",
            "Custo depois da iteração 42400: 0.007453\n",
            "Custo depois da iteração 42500: 0.007436\n",
            "Custo depois da iteração 42600: 0.007418\n",
            "Custo depois da iteração 42700: 0.007400\n",
            "Custo depois da iteração 42800: 0.007383\n",
            "Custo depois da iteração 42900: 0.007365\n",
            "Custo depois da iteração 43000: 0.007348\n",
            "Custo depois da iteração 43100: 0.007330\n",
            "Custo depois da iteração 43200: 0.007313\n",
            "Custo depois da iteração 43300: 0.007296\n",
            "Custo depois da iteração 43400: 0.007279\n",
            "Custo depois da iteração 43500: 0.007262\n",
            "Custo depois da iteração 43600: 0.007245\n",
            "Custo depois da iteração 43700: 0.007228\n",
            "Custo depois da iteração 43800: 0.007211\n",
            "Custo depois da iteração 43900: 0.007194\n",
            "Custo depois da iteração 44000: 0.007178\n",
            "Custo depois da iteração 44100: 0.007161\n",
            "Custo depois da iteração 44200: 0.007145\n",
            "Custo depois da iteração 44300: 0.007128\n",
            "Custo depois da iteração 44400: 0.007112\n",
            "Custo depois da iteração 44500: 0.007096\n",
            "Custo depois da iteração 44600: 0.007079\n",
            "Custo depois da iteração 44700: 0.007063\n",
            "Custo depois da iteração 44800: 0.007047\n",
            "Custo depois da iteração 44900: 0.007031\n",
            "Custo depois da iteração 45000: 0.007015\n",
            "Custo depois da iteração 45100: 0.007000\n",
            "Custo depois da iteração 45200: 0.006984\n",
            "Custo depois da iteração 45300: 0.006968\n",
            "Custo depois da iteração 45400: 0.006952\n",
            "Custo depois da iteração 45500: 0.006937\n",
            "Custo depois da iteração 45600: 0.006921\n",
            "Custo depois da iteração 45700: 0.006906\n",
            "Custo depois da iteração 45800: 0.006891\n",
            "Custo depois da iteração 45900: 0.006875\n",
            "Custo depois da iteração 46000: 0.006860\n",
            "Custo depois da iteração 46100: 0.006845\n",
            "Custo depois da iteração 46200: 0.006830\n",
            "Custo depois da iteração 46300: 0.006815\n",
            "Custo depois da iteração 46400: 0.006800\n",
            "Custo depois da iteração 46500: 0.006785\n",
            "Custo depois da iteração 46600: 0.006770\n",
            "Custo depois da iteração 46700: 0.006756\n",
            "Custo depois da iteração 46800: 0.006741\n",
            "Custo depois da iteração 46900: 0.006726\n",
            "Custo depois da iteração 47000: 0.006712\n",
            "Custo depois da iteração 47100: 0.006697\n",
            "Custo depois da iteração 47200: 0.006683\n",
            "Custo depois da iteração 47300: 0.006668\n",
            "Custo depois da iteração 47400: 0.006654\n",
            "Custo depois da iteração 47500: 0.006640\n",
            "Custo depois da iteração 47600: 0.006626\n",
            "Custo depois da iteração 47700: 0.006611\n",
            "Custo depois da iteração 47800: 0.006597\n",
            "Custo depois da iteração 47900: 0.006583\n",
            "Custo depois da iteração 48000: 0.006569\n",
            "Custo depois da iteração 48100: 0.006556\n",
            "Custo depois da iteração 48200: 0.006542\n",
            "Custo depois da iteração 48300: 0.006528\n",
            "Custo depois da iteração 48400: 0.006514\n",
            "Custo depois da iteração 48500: 0.006501\n",
            "Custo depois da iteração 48600: 0.006487\n",
            "Custo depois da iteração 48700: 0.006473\n",
            "Custo depois da iteração 48800: 0.006460\n",
            "Custo depois da iteração 48900: 0.006446\n",
            "Custo depois da iteração 49000: 0.006433\n",
            "Custo depois da iteração 49100: 0.006420\n",
            "Custo depois da iteração 49200: 0.006406\n",
            "Custo depois da iteração 49300: 0.006393\n",
            "Custo depois da iteração 49400: 0.006380\n",
            "Custo depois da iteração 49500: 0.006367\n",
            "Custo depois da iteração 49600: 0.006354\n",
            "Custo depois da iteração 49700: 0.006341\n",
            "Custo depois da iteração 49800: 0.006328\n",
            "Custo depois da iteração 49900: 0.006315\n",
            "train accuracy: 100.0 %\n",
            "test accuracy: 72.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypjjXLbFPWqe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}